{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction into PyNeuraLogic\n",
    "\n",
    "Learning the XOR operation is a popular elementary task, and serves here as an example to showcase the basics of problem encoding and the library usage. Note that the problem is merely\n",
    "propositional rather than [relational](https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_719) (i.e. we do not yet use logical variables in this problem, causing the rules to correspond to a standard feedforward network rather than a relational model like, e.g., a GNN).\n",
    "\n",
    "\n",
    "The XOR operation has two inputs - $I_1 \\in \\{0, 1\\}$ and $I_2 \\in \\{0, 1\\}$, and one output $O \\in \\{0, 1\\}$. The whole operation can be summarized by Table 1.\n",
    "\n",
    "#### Table 1: The XOR truth table\n",
    "| X | Y | O |\n",
    "|---|---|---|\n",
    "| 0 | 0 | 0 |\n",
    "| 1 | 0 | 1 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 1 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install PyNeuraLogic from PyPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip install neuralogic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Template\n",
    "\n",
    "The model for learning the XOR operation can be expressed\n",
    "in multiple ways; the following model reduces the\n",
    "architecture to one rule, representing one classic neural layer.\n",
    "The rule can be read as: _\"proposition xor is implied by proposition xy.\"_ (the nullary \"relation\" with no arguments corresponds to a proposition, i.e. a simple statement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from neuralogic.nn import get_evaluator\n",
    "from neuralogic.core import Backend, Relation, Template, Settings, Optimizer\n",
    "from neuralogic.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "template = Template()\n",
    "template.add_rule(Relation.xor[1, 8] <= Relation.xy[8, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also declared dimensionality of the weights for each part - $W_{xor}$ for relation `xor` and $W_{xy}$ for relation `xy`. Since we did not specify concrete values for weights here, these will be learnable and sampled randomly from, by default, the uniform distribution (the distribution can be changed via settings)\n",
    "\n",
    "This rule then represents the following equation, where the output of $f(x)$ is the output of the `xor` proposition and $x$ is the value of the `xy` proposition. Functions $\\phi_{rule}$ and $\\phi_{xor}$ are activation functions of our rule and the proposition `xor`, respectively. In our case, $\\phi_{rule}$ is equal to the $\\tanh$ function, and $\\phi_{xor}$ is the identity function.\n",
    "\n",
    "$$W_{xor} \\in \\mathbb{R}^{1, 8}, W_{xy} \\in \\mathbb{R}^{8, 2}, x \\in \\{0,1\\}^2$$\n",
    "\n",
    "$$f(x) = \\phi_{xor}(W_{xor} \\cdot \\phi_{rule}(W_{xy} \\cdot x)) $$\n",
    "\n",
    "\n",
    "## Defining a Dataset\n",
    "\n",
    "To be able to learn our parameters $W_{xor}$\n",
    "and $W_{xy}$, we need to create a training dataset\n",
    "that contains examples. In our case, the dataset\n",
    "examples are straightforward and mimic the truth\n",
    "table (Table 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "\n",
    "\n",
    "dataset.add_examples(\n",
    "    [\n",
    "        Relation.xor[0] <= Relation.xy[[0, 0]],\n",
    "        Relation.xor[1] <= Relation.xy[[0, 1]],\n",
    "        Relation.xor[1] <= Relation.xy[[1, 0]],\n",
    "        Relation.xor[0] <= Relation.xy[[1, 1]],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Each example in the dataset corresponds to one row in the truth table. While defining datasets, the value of each proposition is its actual value and not (a learnable) weight.\n",
    "\n",
    "For example, the following example can be read as: _\"Given the proposition xy's value being equal to the vector $(0, 1)$, we are expecting the proposition xor to have a value equal to scalar $1$.\"_\n",
    "\n",
    "```\n",
    "Relation.xor[1] <= Relation.xy[[0, 1]]\n",
    "```\n",
    "\n",
    "## Training\n",
    "\n",
    "We can do the training manually by writing a training loop, similarly to popular frameworks,  or using a predefined training loop implemented inside evaluators, which are suitable for quick prototyping and switching between different backends, such as DyNet or Java. Such evaluators can be conveniently customized via settings to specify optimizer, learning rate, error function, and more. In our example, we have chosen the Java backend with a stochastic gradient descent optimizer for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss 0.8486421179926945\n",
      "Epoch 10, average loss 0.2523365313646398\n",
      "Epoch 20, average loss 0.22581171403149267\n",
      "Epoch 30, average loss 0.20155291380951051\n",
      "Epoch 40, average loss 0.16632702588300935\n",
      "Epoch 50, average loss 0.12272502214289742\n",
      "Epoch 60, average loss 0.08378590695186636\n",
      "Epoch 70, average loss 0.05659655622355965\n",
      "Epoch 80, average loss 0.03952319346092247\n",
      "Epoch 90, average loss 0.028906304842870375\n"
     ]
    }
   ],
   "source": [
    "printouts = 10\n",
    "\n",
    "settings = Settings(optimizer=Optimizer.SGD, epochs=100)\n",
    "evaluator = get_evaluator(template, settings)\n",
    "built_dataset = evaluator.build_dataset(dataset)\n",
    "\n",
    "for epoch, (total_loss, seen_instances) in \\\n",
    "    enumerate(evaluator.train(built_dataset)):\n",
    "    if epoch % printouts == 0:\n",
    "        print(f\"Epoch {epoch}, average loss {total_loss / seen_instances}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the training is evaluated, our dataset is \"grounded\" with our template. The grounding then yields one computation graph for each query from the dataset. In this propositional problem setting, this will produce, for each query, a computation network with the same structure but with different input and target values.\n",
    "\n",
    "## Testing\n",
    "Evaluators also encapsulate testing with a user-friendly interface that is analogous to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0, predicted: 0\n",
      "Label: 1, predicted: 0.7998393002473414\n",
      "Label: 1, predicted: 0.7991515636502554\n",
      "Label: 0, predicted: 0.0324124386538237\n"
     ]
    }
   ],
   "source": [
    "for label, predicted in evaluator.test(built_dataset):\n",
    "    print(f\"Label: {label}, predicted: {predicted}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
