{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Introduction into PyNeuraLogic\n",
    "\n",
    "Learning the XOR operation is a relatively elementary task, but it serves as a good example to showcase the basics of problem encoding and library usage. Note that the problem is\n",
    "used for simple library introduction and is, in fact,\n",
    "a propositional rather than a relational problem<sup>1</sup>.\n",
    "\n",
    "<sub>[1] i.e. the template here does not contain any variables, causing it to correspond to a standard neural network rather than a GNN.</sub>\n",
    "\n",
    "The XOR operation has two inputs - $I_1 \\in \\{0, 1\\}$ and $I_2 \\in \\{0, 1\\}$, and one output $O \\in \\{0, 1\\}$. The whole operation can be summarized by Table 1.\n",
    "\n",
    "#### Table 1: The XOR truth table\n",
    "| X | Y | O |\n",
    "|---|---|---|\n",
    "| 0 | 0 | 0 |\n",
    "| 1 | 0 | 1 |\n",
    "| 0 | 1 | 1 |\n",
    "| 1 | 1 | 0 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install PyNeuraLogic from PyPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "! pip install neuralogic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Template\n",
    "\n",
    "The model for learning the XOR operation can be expressed\n",
    "in multiple ways; the following model reduces the\n",
    "architecture into one rule, representing one layer.\n",
    "The rule can be read as: _\"Atom xor is implied by atom xy.\"_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from neuralogic.nn import get_evaluator\n",
    "from neuralogic.core import Backend\n",
    "from neuralogic.core import Relation, Template, Initializer, Var, Term\n",
    "from neuralogic.core.settings import Settings, Optimizer\n",
    "from neuralogic.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "template = Template()\n",
    "template.add_rule(Relation.xor[1, 8] <= Relation.xy[8, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also declared weight with given dimensions for each atom - $W_{xor}$ for atom `xor` and $W_{xy}$ for atom `xy`. Since we did not specify concrete values for weights, those learnable parameters will be sampled randomly from, by default, the uniform distribution. <sup>The distribution can be changed via settings.</sup>\n",
    "\n",
    "This rule subsequently represents the following equation, where the output of $f(x)$ is the output of the `xor` atom and $x$ is the value of the `xy` atom. Functions $\\phi_{rule}$ and $\\phi_{xor}$ are activation functions of our rule and the atom `xor`, respectively. In our case, $\\phi_{rule}$ is equal to the $\\tanh$ function, and $\\phi_{xor}$ is the identity function.\n",
    "\n",
    "$$W_{xor} \\in \\mathbb{R}^{1, 8}, W_{xy} \\in \\mathbb{R}^{8, 2}, x \\in \\{0,1\\}^2$$\n",
    "\n",
    "$$f(x) = \\phi_{xor}(W_{xor} \\cdot \\phi_{rule}(W_{xy} \\cdot x)) $$\n",
    "\n",
    "\n",
    "## Defining a Dataset\n",
    "\n",
    "To be able to learn our parameters $W_{xor}$\n",
    "and $W_{xy}$, we need to create a training dataset\n",
    "that contains examples. In our case, the dataset\n",
    "examples are straightforward and mimic the truth\n",
    "table (Table 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "dataset = Dataset()\n",
    "\n",
    "\n",
    "dataset.add_examples(\n",
    "    [\n",
    "        Relation.xor[0] <= Relation.xy[[0, 0]],\n",
    "        Relation.xor[1] <= Relation.xy[[0, 1]],\n",
    "        Relation.xor[1] <= Relation.xy[[1, 0]],\n",
    "        Relation.xor[0] <= Relation.xy[[1, 1]],\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Each example in the dataset corresponds to one row in the truth table. In the scope of datasets, the value of each atom is its actual value and not (a learnable) weight.\n",
    "\n",
    "For example, the following example can be read as: _\"Given the atom xy's value is equal to the vector $(0, 1)$, we are expecting the atom xor to have a value equal to scalar $1$.\"_\n",
    "\n",
    "```\n",
    "Relation.xor[1] <= Relation.xy[[0, 1]]\n",
    "```\n",
    "\n",
    "## Training\n",
    "\n",
    "We can do the training manually by writing a training loop, similarly to popular frameworks,  or using a predefined training loop implemented inside evaluators, which are suitable for quick prototyping and switching between different backends, such as DyNet or Java. Such evaluators can be conveniently customized via settings to specify optimizer, learning rate, error function, and more. In our example, we have chosen the Java backend with a stochastic gradient descent optimizer for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, average loss 0.2553116771217217\n",
      "Epoch 10, average loss 0.23218485887389556\n",
      "Epoch 20, average loss 0.22260444774242782\n",
      "Epoch 30, average loss 0.21021060680956824\n",
      "Epoch 40, average loss 0.19171042395017732\n",
      "Epoch 50, average loss 0.16472744948159912\n",
      "Epoch 60, average loss 0.13077879208603532\n",
      "Epoch 70, average loss 0.09641379057845617\n",
      "Epoch 80, average loss 0.06837290097910131\n",
      "Epoch 90, average loss 0.04863894701863611\n"
     ]
    }
   ],
   "source": [
    "settings = Settings(optimizer=Optimizer.SGD, epochs=100)\n",
    "\n",
    "evaluator = get_evaluator(template, Backend.JAVA, settings=settings)\n",
    "\n",
    "printouts = 10\n",
    "\n",
    "for epoch, (total_loss, seen_instances) in \\\n",
    "    enumerate(evaluator.train(dataset)):\n",
    "    if epoch % printouts == 0:\n",
    "        print(f\"Epoch {epoch}, average loss {total_loss / seen_instances}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before the training is evaluated, our dataset is grounded with our template. The grounding then yields one computation graph for each query from the dataset. In our case this will produce, for each query, a computation network with the same structure but with different input and target values.\n",
    "\n",
    "## Testing\n",
    "Evaluators also encapsulate testing with a user-friendly interface that is analogous to training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 0.0, predicted: 0.0\n",
      "Label: 1.0, predicted: 0.7574968842735786\n",
      "Label: 1.0, predicted: 0.7426854473631991\n",
      "Label: 0.0, predicted: 0.05052201839625794\n"
     ]
    }
   ],
   "source": [
    "for label, predicted in evaluator.test(dataset):\n",
    "    print(f\"Label: {label}, predicted: {predicted}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
